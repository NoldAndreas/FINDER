#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''

Trains and validates new models on training & validation data generated by
stage 2.0_Pool_Trn_and_Val_Sets.py

Existing models can be re-validated again here too.

@author: dave
'''

import os
import numpy as np
import pickle
import matplotlib.pyplot as plt
import csv
import time

# Keras and Tensorflow
import keras
from keras.models import load_model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM # Long Short-Term Memory (LSTM)
from keras.layers import Dropout
from keras.layers import Flatten
#from keras.layers import TimeDistributed
from keras.layers.convolutional import Conv1D # import convnet
from keras.layers.convolutional import MaxPooling1D # import convnet
from keras.callbacks import ModelCheckpoint # save partially trained models after each epoch
from keras.utils import to_categorical
#from keras.layers.convolutional import MaxPooling2D # import convnet
#from keras.layers.embeddings import Embedding
#from sklearn import preprocessing
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix

from keras import backend as K # to verify we are using the GPU 

proc_wd = os.path.dirname(os.path.abspath(__file__))
if os.getcwd() != proc_wd:
    os.chdir(proc_wd)
    print('Changed working directory to ' + proc_wd)

import FuncEtc as fn_etc
import FuncNormalizeInput as fn_normalize


# ========================================================================
#  Preparing data for the model
# ========================================================================

# Models are constructed from data prepared according to the option chosen here:

# PreparationType = 'Raw-Distances'              # The absolute NN distances
# PreparationType = 'Norm-Distances-Only'        # l2 normalisation of the raw distances

# PreparationType = 'Raw-Differences'            # The difference between each consective NN distance
# PreparationType = 'Diff-Norm'                  # Consecutive differences are calculated and then normalised (using l2 norm). Values are then rescaled by 100000
# PreparationType = 'Norm-Diff'                  # Default normalisation (l2 norm, i.e. sqrt(sum(abs(X).^2)) ) is applied. Values are rescaled by 100000 and the consecutive differences taken.
# PreparationType = 'Norm-Diffs-Only-NoRescale'  # As above but without rescaling the norm values.

PreparationType = 'Norm-Self'                   # Normalize each point's distance-differences to it's own min/max
# PreparationType = 'Norm-FirstDiff'             # The first (nearest) distance is used to normalise the other distance-differences
# PreparationType = 'Norm-MinDiff'               # The minimum distance-hop is used to normalise the other distance-differences
# PreparationType = 'Norm-MaxDiff'               # The maximum distance-hop is used to normalise the other distance-differences

# PreparationType = 'd2NN_SelfNormalized'        # TODO

# PreparationType = 'CoordsOnly'                   # our input are the raw coordinates for each NN


# ========================================================================
#  Converting scores to labels
# ========================================================================

pred_threshold = 0.5 # points need to be above this score to qualify as clustered (normally 0.5).
# This is only for binary classification models (e.g. clustered or not-clustered)
# For multiple classification, the label with the highest score is assigned to the point.


# ========================================================================
#  Training settings
# ========================================================================

# The training (et al.) datasets can't go through the model all at once so they are
# split into batches. Each batch has this many examples (points), processed together.
mdl_batchsize = 32

# Epoch is one run through the entire set of training data
mdl_epochs = 100

# Learning rate is how much the network changes internally (weights and biases) after each epoch
# the learning rate can be constant or it can change as the epochs progress.
mdl_learning_rate = 0.001 # initial learning rate
mdl_epoch_factor = 1.0    # influence of epochs on learning rate decay. Must be greater than zero!
mdl_decay_rate = mdl_learning_rate / (mdl_epoch_factor * mdl_epochs) # zero = no change in learning rate; it's always mdl_learning_rate for all epochs.

# The optimizer function. ADAM works very well but you may want to try something else here.
opt_adam = keras.optimizers.Adam(lr=mdl_learning_rate, decay=mdl_decay_rate)


# ========================================================================
#  Defining the model
# ========================================================================

# Descriptions about the data
mdl_features_length = 100  # how many samples (near-neighbours) do we have for our input data?
mdl_features_types = 1      # how many different features does our input data have?
                            # e.g. 1 = Distances only, 2 = x and y coords (or 2 = distances and angles)
mdl_labels = 1              # how many labels do we need to find, i.e. size of the final output?
                            # 1 = when doing binary classification, e.g. for two labels (not-clustered, clustered) 
                            # 3 = when doing multiple classifications, e.g. for three labels (non-clustered, clustered(round), clustered(fibre)), etc.

# Shape of features to be fed into the model is calculated here and used later for
# defining the model layers. You don't need to edit this if you have given values
# for mdl_features_length and mdl_features_types!
input_data_shape = (mdl_features_length, mdl_features_types) 

##
## Model layer arrangements are shown below. Comment and un-comment blocks as
## needed to hide and reveal configurations that you want to use. A few examples
## are given below.
##

# CAML 'simplest' model configuration, for binary labels (e.g. non-clustered, clustered)

def build_model():
    classifier_model = Sequential()
    classifier_model.add(Dense(32, activation='relu', input_shape=input_data_shape))
    classifier_model.add(Flatten())
    classifier_model.add(Dense(mdl_labels, activation='sigmoid'))
    return classifier_model


## CAML 'expanded' configuration (11 layers)
#
#def build_model():
#    classifier_model = Sequential()
#    classifier_model.add(Conv1D(filters=32, kernel_size=3, padding='valid', strides=1, activation='relu', input_shape=input_data_shape))
#    classifier_model.add(MaxPooling1D(pool_size=4))
#    classifier_model.add(Dropout(0.2))
#    classifier_model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))
#    classifier_model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))
#    classifier_model.add(Dropout(0.2))
#    classifier_model.add(MaxPooling1D(pool_size=4))
#    classifier_model.add(Flatten())
##    classifier_model.add(Dense(100, activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='RandomUniform'))
#    classifier_model.add(Dense(32, input_dim=60, kernel_initializer='normal', activation='relu'))
#    classifier_model.add(Dense(16, kernel_initializer='normal', activation='relu'))
#    classifier_model.add(Dense(mdl_labels, activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='RandomUniform'))
#    return classifier_model


## CAML simple configuration for multilabel data (e.g. non-clustered, round-clustered, fiber-clustered)
## Main difference here is the activation function; Sigmoid works better for binary classification but
## we use a softmax here as we expect multiple classes (and want scores for each class).
#
#def build_model():
#    classifier_model = Sequential()
#    classifier_model.add(Dense(32, activation='relu', input_shape=input_data_shape))
#    classifier_model.add(Flatten())
#    classifier_model.add(Dense(mdl_labels, activation='softmax'))
#    return classifier_model


# ========================================================================
#  Saving the model
# ========================================================================
# This allows you to recover a training session which is interrupted, for example
# because the power goes off in your building quite regularly and without notice,
# and nobody in Facilities Management really knows why or does anything to fix it.
    
auto_save_model = False        # Save a partially trained model after every epoch.
delete_autosaved_models = True # Delete partial models once all epochs complete.
log_to_csv = True              # Per-epoch performance can be saved to a CSV file.


# ========================================================================
#  Plotting variables
# ========================================================================
# Specify a colourmap for scatter plots. You can supply a text file with RGB values
# or give the name of a matplotlib built-in colormap. You can use 'jet' but you'll
# get a warning about it.
cmapfile = 'viridis'
#cmapfile = './cmaps/candybright_256_rgb.txt'
#cmapfile = './cmaps/DavLUT.lut'
# matplotlib built-in colormap names are also valid. For more information see
# https://matplotlib.org/3.1.1/tutorials/colors/colormaps.html

plotbgcolor = (0.1765, 0.1765, 0.1765)    # Background color for scatter plots
plottxtcolor = (1, 1, 1)                  # Text color for scatter plots

ExportImageFormat = ['eps','png'] # file extension(s) of the saved images. Use 'eps', 'png', 'jpg', 'svg'.


# ========================================================================
#  End of user-editable variables
# ========================================================================

if __name__ == '__main__':
    
#    # set the random seed
#    todays_seed = 270879
#    np.random.seed(todays_seed)

    if cmapfile == 'jet':
        fn_etc.warn_msg('You have specified the \'jet\' colormap, with its infamous perceptual ambiguity problems.\r\n'+
                        'You should consider selecting a perceptually uniform colormap such as:\r\n'+
                        '\t- Viridis\r\n'+
                        '\t- Plasma\r\n'+
                        '\t- Inferno\r\n'+
                        '\t- Magma\r\n'+
                        '\t- Cividis\r\n')

    cmap = fn_etc.load_colormap(cmapfile, flip=False)
    
    # ========================================================================
    #  Get the input data
    # ========================================================================

    # Does training data exist from previous 1_DataPreparation script?
#    newPKLFile = True
#    if 'TrainValTest_data_fname' in locals() or 'inputpkl' in locals():
#        try:
#            previousdata = TrainValTest_data_fname
#        except:
#            previousdata = inputpkl
#        
#        if 'recall_data' in locals():
#            recycleRecallData = fn_etc.askforinput(
#                message = 'Do you want to re-use the raw (distances) training, validation, and testing data (loaded from ' + previousdata + ')?',
#                errormessage = 'Please enter y or n',
#                defaultval = default_input_pkl,
#                isvalid = lambda v : v.lower() in ['y','n','yes','no'])
#            
#            if recycleRecallData.lower() in ['y','yes']:
#                newPKLFile = False
#                fn_etc.ok_msg('Using the existing training, validation, and testing data.')
#
#    if newPKLFile:
    if 'inputpkl' in locals() and os.path.exists(inputpkl):
        # recycle input PKL from a previous run of this script
        default_input_pkl = inputpkl
    elif 'TrainValTest_data_fname' in locals() and os.path.exists(TrainValTest_data_fname):
        # use a PKL saved from a recently run (in this session) Stage 2 script
        default_input_pkl = TrainValTest_data_fname
    else:
        default_input_pkl = ''
    
    inputpkl = fn_etc.askforinput(
        message = 'PKL file containing pooled training, validation, and testing data from Stage 2',
        errormessage = 'The file you provided does not exist or you have given a folder name instead of a file name.',
        defaultval = default_input_pkl,
        isvalid = lambda v : os.path.isfile(v))

    inputfolder = os.path.dirname(inputpkl)
   
    # ========================================================================
    #  Import training, validation, and testing data
    # ========================================================================
    #
    ### Load existing training, validation, and testing data pool

    print('Importing PKL file...')
    
    # load items from our pickle and create variables based on the variable name
    # stored in the first pickled item (the list of variable names)
    pickle_open_file = open(inputpkl, 'rb' ) # open the file for reading
    load_order = pickle.load(pickle_open_file) # load first item: the list of names
    for item in load_order:
        locals()[item] = pickle.load(pickle_open_file) # load each and name them
    pickle_open_file.close()
    
    # The following variables will already have been loaded by the previous 
    # statement but given here in an inaccessible statement to show what has been
    # unpickled.
    loaded_pickle = True # this is to avoid dropping into the if statement below
    if not loaded_pickle:
        total_training_count = None          # These variables have been loaded
        training_label_fraction = None       # from the pickle file, above.
        training_reqd_counts = None          #
        X_training_distances_raw = None      # Normally we have to recall the
        Y_training_vectors_raw = None        # names in the exact same order
        Y_training_binary_raw = None         # they were saved but above we used
        Y_training_labels_raw = None         # a list (loaded from the same
        total_validation_count = None        # pickle file) to know the names
        validation_label_fraction = None     # and the order they were saved.
        validation_reqd_counts = None        #
        X_validation_distances_raw = None    # In case you are trying to search
        Y_validation_vectors_raw = None      # for the names to find where they
        Y_validation_binary_raw = None       # originate they are stated here
        Y_validation_labels_raw = None       # explicitly but in an 'if' which
        total_testing_count = None           # should never be executed.
        testing_label_fraction = None        #
        testing_reqd_counts = None           # Their explict statement here
        X_testing_distances_raw = None       # should also help remove syntax
        Y_testing_vectors_raw = None         # errors in your IDE.
        Y_testing_binary_raw = None          # 
        Y_testing_labels_raw = None          # If changing these variable names,
        ps = None                            # also edit the script for Stage 2
        label_names = None                   # (pooling training data) processing.

    fn_etc.ok_msg('Training, validation, and testing data were unpickled from ' + inputpkl)

    # check that the unpickled data is suitable for our model specifications
    if X_training_distances_raw.shape[1] != mdl_features_length:
        
        fn_etc.err_msg('You specified ' + str(mdl_features_length) + ' near-neighbours (variable \'mdl_features_length\' in this script) but the imported training data is for ' + str(X_training_distances_raw.shape[1]) + ' neighbours...')
        
        if mdl_features_length < X_training_distances_raw.shape[1]:
            recheck_features_len = fn_etc.askforinput(
                            message = 'Do you want to [1]-Cancel now or [2]-Update \'mdl_features_length\' to match the imported training data or [3] Trim the imported data to match the expected size? (Enter 1 or 2 or 3)',
                            errormessage= 'Type the number 1 or 2 or 3 and press enter',
                            defaultval= '1',
                            isvalid = lambda v : v in ['1','2','3'])
        else: 
            recheck_features_len = fn_etc.askforinput(
                            message = 'Do you want to [1]-Cancel now or [2]-Update \'mdl_features_length\' to match the imported training data? (Enter 1 or 2)',
                            errormessage= 'Type the number 1 or 2 and press enter',
                            defaultval= '1',
                            isvalid = lambda v : v in ['1','2'])
    
        if recheck_features_len in ['1']:
            
            raise ValueError('Please check that the size of your training data matches the \'mdl_features_length\' setting in this script (or change either, as desired).')
            
        elif recheck_features_len in ['2']:
            
            mdl_features_length = X_training_distances_raw.shape[1]
            input_data_shape = (mdl_features_length, mdl_features_types) # this is used when building the model so it needs updating too.
            fn_etc.info_msg('Changed variable \'mdl_features_length\' to ' + str(mdl_features_length) + ' near-neighbours')
            
        elif recheck_features_len in ['3']:
            
            fn_etc.info_msg('Trimming the original input data pool from ' + str(X_training_distances_raw.shape[1]) + ' near-neighbours to match \'mdl_features_length\' (' + str(mdl_features_length) + ' near-neighbours)')
            
            X_training_distances_raw2 = X_training_distances_raw[:,:mdl_features_length,:]
            X_validation_distances_raw = X_validation_distances_raw[:,:mdl_features_length,:]
            X_testing_distances_raw = X_testing_distances_raw[:,:mdl_features_length,:]

    if len(X_training_distances_raw.shape) > 2 and X_training_distances_raw.shape[2] != mdl_features_types:
        
        fn_etc.err_msg('You specified ' + str(mdl_features_types) + ' feature(s) but the training data was done with ' + str(X_training_distances_raw.shape[2]) + ' feature(s)...')
        
        recheck_features_type = fn_etc.askforinput(
                        message = 'Do you want to [1]-Cancel now or [2]-Update configuration to match the training data? (Enter 1 or 2)',
                        errormessage= 'Type the number 1 or 2 and press enter',
                        defaultval= '1',
                        isvalid = lambda v : v in ['1','2'])
    
        if recheck_features_type in ['1']:
            
            raise ValueError('Please give training data which matches the settings in this script.')
            
        elif recheck_features_type in ['2']:
            
            mdl_features_types = X_training_distances_raw.shape[1]
            fn_etc.info_msg('Changed variable \'mdl_features_types\' to support ' + str(mdl_features_types) + ' feature types')


    # ========================================================================
    #  Data Preparation: Turn raw input data (near-neighbour distances)
    #  into input for the model (normalized distances or whatever was chosen
    #  for PreparationType.
    # ========================================================================
    if 'X_training_data' and 'X_validation_data' and 'X_testing_data' in locals():
        recyclePreparedData = fn_etc.askforinput(
            message = 'Existing prepared data (by ' + PreparationType + ' method) for training, validation, and testing exists. Do you want to re-use it?',
            errormessage = 'Please enter y or n',
            defaultval = default_input_pkl,
            isvalid = lambda v : v.lower() in ['y','n','yes','no'])
        
        if recyclePreparedData.lower() in ['n','no']:
            RedoDataPrep = True
        else:
            RedoDataPrep = False
            fn_etc.ok_msg('Training, validation, and testing data will be re-used. NB: These data were prepared as \'' + PreparationType + '\'')
            
    else:
        RedoDataPrep = True

    if RedoDataPrep:
        checkPrepType = fn_etc.askforinput(
            message = 'This script has Preparation Type set to ' + PreparationType + ' ... is this correct (Enter yes or no)?\nEntering no will allow you to change the type: ',
            errormessage = 'Please enter y or n',
            defaultval = default_input_pkl,
            isvalid = lambda v : v.lower() in ['y','n','yes','no'])
        
        if checkPrepType.lower() in ['n','no']:
            ValidPreparationTypes = list(['Raw-Distances',
                                         'Norm-Distances-Only',
                                         'Raw-Differences',
                                         'Diff-Norm',
                                         'Norm-Diff',
                                         'Norm-Diffs-Only-NoRescale',
                                         'Norm-Self',
                                         'Norm-FirstDiff',
                                         'Norm-MinDiff',
                                         'Norm-MaxDiff',
                                         'CoordsOnly'])
            adjustPrepType = fn_etc.askforinput(
                message = 'Enter the PreparationType value for the training data:',
                errormessage = 'Please enter a valid PreparationType (check this script for a list of valid types)',
                defaultval = default_input_pkl,
                isvalid = lambda v : v in ValidPreparationTypes)
            
            fn_etc.info_msg('Changed PreparationType from ' + PreparationType + ' to ' + adjustPrepType)
            PreparationType = adjustPrepType
            
        X_training_data = fn_normalize.normalize_dists(X_training_distances_raw, total_training_count, PreparationType)
        X_validation_data = fn_normalize.normalize_dists(X_validation_distances_raw, total_validation_count, PreparationType)
        X_testing_data = fn_normalize.normalize_dists(X_testing_distances_raw, total_testing_count, PreparationType)
        
        # training on the labels
        Y_training_data = Y_training_labels_raw
        Y_validation_data = Y_validation_labels_raw
        Y_testing_data = Y_testing_labels_raw

        if mdl_labels > 1: # len(label_names) > 2
            # convert our integer training labels into binary encoded
            Y_training_data = to_categorical(Y_training_data)
            Y_validation_data = to_categorical(Y_validation_data)
            Y_testing_data = to_categorical(Y_testing_data)

#        # alternative training Y sets ... from the binary labels or trying to guess the 'length' of the neighbours-in-same-cluster vector
#        # training by simple binary labels (0 or 1)
#        Y_training_data = Y_training_binary_raw
#        Y_validation_data = Y_validation_binary_raw
#        Y_testing_data = Y_testing_binary_raw
        
#        # training by vector representing clustered-alike neighbours
#        Y_training_data = Y_training_vectors_raw
#        Y_validation_data = Y_validation_vectors_raw
#        Y_testing_data = Y_testing_vectors_raw
        
        # reshape data to be repeats/measures/features for LSTM.
        # The shape ought to be three dimensions (total-samples, length-of-feature, total-features)
        # If it's not already 3D (e.g. single-feature data) then we can enforce it here.
        if X_training_data.ndim != 3:
            X_training_data = X_training_data.reshape((total_training_count, ps['FurthestFriend'], mdl_features_types))
        if X_validation_data.ndim != 3:
            X_validation_data = X_validation_data.reshape((total_validation_count, ps['FurthestFriend'], mdl_features_types))
        if X_testing_data.ndim != 3:
            X_testing_data = X_testing_data.reshape((total_testing_count, ps['FurthestFriend'], mdl_features_types))
    # end of redo-dataprep

    # ========================================================================
    #  Model Training
    # ========================================================================

#    # for F1 score   
#    class Metrics(keras.callbacks.Callback):
#        def on_epoch_end(self, batch, logs={}):
#            predict = np.asarray(self.model.predict(self.validation_data[0]))
#            targ = self.validation_data[1]
#            self.f1s = f1(targ, predict)
#            return
#    metrics = Metrics()

    # List available GPUs
    GPU_list = K.tensorflow_backend._get_available_gpus()
    if len(GPU_list) > 0:
        if len(GPU_list) == 1:
            fn_etc.info_msg('The following GPU is available: ')
        else:
            fn_etc.info_msg('The following GPUs are available: ')
        for GPUavailable in GPU_list:
            print('\t\t' + GPUavailable)
    else:
        fn_etc.info_msg('No GPUs available; processing will be done on CPU(s)')
        
    # ask if we need to train a new model or load a new one (for validation of an existing model etc)
    new_load_model = fn_etc.askforinput(
        message = 'Do you want to [1]-Train a new model or [2]-Load an existing model? (Enter 1 or 2)',
        errormessage= 'Type the number 1 or 2 and press enter',
        defaultval= '1',
        isvalid = lambda v : v in ['1','2'])
        
    if new_load_model in ['1']:
    
        terrible_name = True
        while terrible_name:
            modelID = fn_etc.id_generator()
            
            check_name_ok = fn_etc.askforinput(
                    message = 'Model will be named ' + modelID + ' - is this OK? (Y or N)',
                    errormessage= 'Type Y or N',
                    defaultval= 'y',
                    isvalid = lambda v : v.lower() in ['y','n','yes','no'])
    
            if check_name_ok.lower() in ['y','yes']:
                terrible_name = False
                print('Model name accepted.')
            else:
                print('Model name rejected.')
            
        modelID_full = modelID + ' - ' + PreparationType + ' - Train(' + str((round(total_training_count/100)/10)) + 'k) - Val(' + str((round(total_validation_count/100)/10)) + 'k)'
        default_model_path = os.path.abspath(os.path.join(inputfolder, '..', '3_models', modelID + '_(' + PreparationType + ')'))
        print('New model \'' + modelID + '\' will be trained!')
        
        outputpath_model = fn_etc.askforinput(
                message = 'Path to save new model \'' + modelID + '\'',
                errormessage= 'An output folder must be given; if the path does not exist it will be created.',
                defaultval= default_model_path,
                isvalid = lambda v : len(v) > 0 and not v.isspace())
                
        #make the folder to hold all models generated by these data
        if not os.path.exists('../3_models'):
            os.makedirs('../3_models')
        
        #make the folder for the output data
        if not os.path.exists(outputpath_model):
            os.makedirs(outputpath_model)
        
        this_model_notes = fn_etc.askforinput(
                message = 'Enter any custom quick notes here, to be saved alongside Model ' + modelID + '.\nLeave empty (just press enter) to save no notes.\n\tYour notes:\n\t',
                errormessage= 'You should never see this error message as everything is accepted at this input.',
                defaultval= '',
                isvalid = lambda v : len(v) >= 0)
        
        if len(this_model_notes) == 0:
            this_model_notes = '(none supplied)'
            
        model_notes_file = os.path.join(outputpath_model, modelID + ' - notes.txt')
        with open(model_notes_file, 'w') as text_file:
            text_file.write('---- ' + modelID + '- Training Notes ----\n')
            text_file.write('Training Pool Source: ' + inputpkl + '\n')
            text_file.write('Data preparation method: ' + PreparationType + '\n')
            text_file.write('---- User-supplied notes: ----\n')
            text_file.write(this_model_notes + '\n')

        proceed_with_processing = fn_etc.askforinput(
            message = 'When you are ready to proceed press Enter (or X to cancel everything and exit)',
            errormessage= 'Type P to proceed or X to exit',
            defaultval= 'P',
            isvalid = lambda v : v.lower() in ['p','x'])
        
        if proceed_with_processing.lower() in ['p']:
            print('Rightyo, off we go...\n')
        elif proceed_with_processing.lower() in ['x']:
            print('That\'s ok. Maybe next time?')
            raise ValueError('No errors, you just decided not to proceed and that\'s OK! :)')


        model = build_model()
        if mdl_labels == 1:
            model.compile(loss='binary_crossentropy', optimizer=opt_adam, metrics=['accuracy'])
        else:
            model.compile(loss='categorical_crossentropy', optimizer=opt_adam, metrics=['accuracy'])
        print(model.summary())
        # print(model.get_config())
        
        # save model config to JSON and readable text
        model_json_export = model.to_json(indent=4)
        model_summary_fname = os.path.join(outputpath_model, modelID_full)
        
        with open(model_summary_fname + ' - Model Config.json','w+') as f:
            print(model_json_export, file=f)
        
        with open(model_summary_fname + ' - Model Summary.txt','w') as fh:
            # Pass the file handle in as a lambda function to make it callable
            model.summary(print_fn=lambda x: fh.write(x + '\n'))
        
        with open(model_summary_fname + ' - Model Summary.txt', 'a') as fh:
            # add some extra information
            print('Batch size:\t' + str(mdl_batchsize), file=fh)
            print('Epochs:\t' + str(mdl_epochs), file=fh)
            print('Initial learning rate:\t' + str(mdl_learning_rate), file=fh)
            print('Learning rate decay epoch factor:\t' + str(mdl_epoch_factor), file=fh)
            print('Learning rate decay function value:\t' + str(mdl_decay_rate), file=fh)

        # Callbacks to save the model after each epoch and do other useful things.
        # For many other useful callbacks see https://keras.io/callbacks/
        callbacks_list = [] # we can pass several callbacks, our list starts out empty.

        if  auto_save_model:
            model_partial_base='_Partial_Epoch-{epoch:02d}_ValAcc-{val_acc:.2f}_ValLoss-{val_loss:.2f}'
            model_partial_fname = os.path.join(outputpath_model, modelID + model_partial_base + '.h5')
            # can save for every epoch or can only save models which perform better than before
            ## This will save models only if they have better performance than previous epochs
            # checkpoint = ModelCheckpoint(model_partial_fname, monitor='val_acc', verbose=0, save_best_only=True, mode='max')
            # This will save the model after every epoch, regardless of performance improvement
            checkpoint = ModelCheckpoint(model_partial_fname, verbose=0)
            callbacks_list.append(checkpoint)

        if log_to_csv:
            csv_logger = keras.callbacks.CSVLogger(os.path.join(outputpath_model, modelID + '_training_log.csv'))
            callbacks_list.append(csv_logger)

#        # to recover a partially trained model:
#        # load weights
#        model_partial_fname = '/path/to/the/partially-trained-model.h5'
#        model.load_weights(model_partial_fname)
#        epochs_done = 57 # wherever we got up to with the saved weights
#        # Compile model with the loaded weights, same as earlier
#        if mdl_labels == 1:
#            model.compile(loss='binary_crossentropy', optimizer=opt_adam, metrics=['accuracy'])
#        else:
#            model.compile(loss='categorical_crossentropy', optimizer=opt_adam, metrics=['accuracy'])#        
#        # Reset the epochs to however many you want to carry on with
#        mdl_epochs = mdl_epochs - epochs_done

        # train the model
        training_history = model.fit(X_training_data, 
                                     Y_training_data,
                                     batch_size=mdl_batchsize, 
                                     epochs=mdl_epochs,
                                     validation_data=(X_validation_data, Y_validation_data),
                                     callbacks=callbacks_list)

        if delete_autosaved_models:
            all_files = os.listdir(outputpath_model)
            for tmp_file in all_files:
                if tmp_file.endswith('.h5'):
                    os.remove(os.path.join(outputpath_model, tmp_file))
            
        model_fname = os.path.join(outputpath_model, modelID_full + '.h5')
        model.save(model_fname, include_optimizer=False)  # saves model as HDF5 file.
        # If you wish to train the model further then you can include the optimizer
        # state when saving the model, by setting include_optimizer=True
        # Including the optimizer can cause loading problems downstream but 
        # hopefully this will be fixed in time with later versions of Keras. Leaving
        # the optimizer out will cause warnings but they can be ignored.
        
        # model_weights_export = model.save_weights() # save the trained weights separately
        
        # To show what's available in training_history ... 
        #print(training_history.history.keys())
        #print(training_history.history['loss'])
        #print(training_history.history['accuracy'])
        #print(training_history.history['val_loss'])
        #print(training_history.history['val_accuracy'])
        
        # plot the training and validation accuracy over epochs
        fig_TrnVal_Accu = plt.figure(facecolor = plotbgcolor)
        ax_TrnVal_Accu = fig_TrnVal_Accu.add_subplot(111, facecolor = plotbgcolor)
        fig_TrnVal_Accu.set_size_inches(10, 10)
        
#        plt.plot(training_history.epoch, training_history.history['accuracy'], color='orange', alpha=0.5)
#        plt.plot(training_history.epoch, training_history.history['val_accuracy'], color='cyan', alpha=0.5)
#        plt.scatter(training_history.epoch, training_history.history['accuracy'], color='orange', s=1)
#        plt.scatter(training_history.epoch, training_history.history['val_accuracy'], color='cyan', s=1)
        plt.plot(training_history.epoch, training_history.history['accuracy'], color='orange', alpha=0.5)
        plt.plot(training_history.epoch, training_history.history['val_accuracy'], color='cyan', alpha=0.5)
        plt.scatter(training_history.epoch, training_history.history['accuracy'], color='orange', s=1)
        plt.scatter(training_history.epoch, training_history.history['val_accuracy'], color='cyan', s=1)

        # pin axis limits
    #    plt.xlim((0.0,mdl_epochs))
        plt.ylim((0.5,1.0))
        # fix axis colours
        ax_TrnVal_Accu.tick_params(color=plottxtcolor, labelcolor=plottxtcolor)
        for spine in ax_TrnVal_Accu.spines.values():
            spine.set_edgecolor(plottxtcolor)
        # add legend and match colours
        leg = plt.legend(['Training', 'Validation'], loc='lower right', facecolor = plotbgcolor)
        for handle, text in zip(leg.legendHandles, leg.get_texts()):
            text.set_color(handle.get_color())
        # add labels etc
        plt.title('Model ' + modelID + ' training & validation accuracy', color=plottxtcolor)
        plt.ylabel('Accuracy', color=plottxtcolor)
        plt.xlabel('Epoch', color=plottxtcolor)    
        fig_TrnVal_Accu.tight_layout()
        plt.show()
        # save it
        for ExportImage in ExportImageFormat:
            if ExportImage == 'png':
                transparent_bg = True
            else:
                transparent_bg = False
            fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Training & Validation Accuracy Evolution.' + ExportImage)
            fig_TrnVal_Accu.savefig(fname_out,
                        dpi=300,
                        bbox_inches=0,
                        facecolor=plotbgcolor,
                        edgecolor='none',
                        transparent=transparent_bg)
        plt.close()

        
        # plot the training and validation loss over epochs
        fig_TrnVal_Loss = plt.figure(facecolor = plotbgcolor)
        ax_TrnVal_Loss = fig_TrnVal_Loss.add_subplot(111, facecolor = plotbgcolor)
        fig_TrnVal_Loss.set_size_inches(10, 10)
        
        plt.plot(training_history.epoch, training_history.history['loss'], color='orange', alpha=0.5)
        plt.plot(training_history.epoch, training_history.history['val_loss'], color='cyan', alpha=0.5)
        plt.scatter(training_history.epoch, training_history.history['loss'], color='orange', s=1)
        plt.scatter(training_history.epoch, training_history.history['val_loss'], color='cyan', s=1)
        # pin axis limits
    #    plt.xlim((0.0,mdl_epochs))
        loss_y_max = np.ceil(np.max([training_history.history['loss'], training_history.history['val_loss']]) * 10) / 10
        plt.ylim((0, np.max((loss_y_max, 0.5))))
        # fix axis colours
        ax_TrnVal_Loss.tick_params(color=plottxtcolor, labelcolor=plottxtcolor)
        for spine in ax_TrnVal_Loss.spines.values():
            spine.set_edgecolor(plottxtcolor)
        # add legend and match colours
        leg = plt.legend(['Training', 'Validation'], loc='upper right', facecolor = plotbgcolor)
        for handle, text in zip(leg.legendHandles, leg.get_texts()):
            text.set_color(handle.get_color())
        # add labels etc
        plt.title('Model ' + modelID + ' training & validation loss', color=plottxtcolor)
        plt.ylabel('Loss', color=plottxtcolor)
        plt.xlabel('Epoch', color=plottxtcolor)    
        fig_TrnVal_Loss.tight_layout()
        plt.show()
        # save it
        for ExportImage in ExportImageFormat:
            if ExportImage == 'png':
                transparent_bg = True
            else:
                transparent_bg = False
            fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Training & Validation Loss Evolution.' + ExportImage)
            fig_TrnVal_Loss.savefig(fname_out,
                        dpi=300,
                        bbox_inches=0,
                        facecolor=plotbgcolor,
                        edgecolor='none',
                        transparent=transparent_bg)
        plt.close()

        # ========================================================================
        #    Draw a schematic outline of the model
        # ========================================================================
        
        from keras.utils import plot_model
        plot_model(model, to_file=os.path.join(outputpath_model, modelID_full + '.png'), show_shapes=True) # Save the model diagram as png file


    elif new_load_model in ['2']:
        
        # Load an existing (trained) model and run the validation stuff on it, e.g.
        # to validation on different datasets prepared by Stage 2.
        if 'model_fname' in locals():
            # model_fname exists from previous script
            default_model_file = model_fname
        else:
            default_model_file = ''
    
        model_fname = fn_etc.askforinput(
                message = 'Path to the model file (.h5 file)',
                errormessage= 'The file you provided does not exist or you have supplied a folder instead of a file.',
                defaultval= default_model_file,
                isvalid = lambda v : os.path.isfile(v))
        
        # load the model and set up output folders
        if os.path.exists(model_fname):
            model = load_model(model_fname)

            model_home_folder = os.path.dirname(model_fname)
            timestamp = time.strftime('%y%m%d_%H%M', time.localtime())
            outputpath_model = os.path.join(model_home_folder, timestamp + ' - testing')
            
            # extract the model ID
            modelID_full = os.path.splitext(os.path.basename(os.path.normpath(model_fname)))[0]
            modelID = modelID_full.split()[0]
            model_summary_fname =  os.path.join(outputpath_model, modelID + ' - ' + timestamp)

            # get some showstopping parameters from the model
            model_config = model.get_config()
            RequiredInputSize = model_config[0]['config']['batch_input_shape'][1]
            ModelLabelsTotal = model_config[-1]['config']['units']
            
            if X_training_data.shape[1] != RequiredInputSize:
                raise ValueError('Please make sure your model and input data sizes are matched.\r\n' + 
                                 'Model ' + modelID + ' expects ' + str(RequiredInputSize) + ' near-neighbour values.\r\n' + 
                                 'Your input data has ' + str(X_training_data.shape[1]) + ' near-neighbour values.\r\n' + 
                                 'Either choose a different model or a choose different training data pool.')
        
            if not os.path.exists(outputpath_model):
                os.makedirs(outputpath_model)

            if ModelLabelsTotal == 1:
                model.compile(loss='binary_crossentropy', optimizer=opt_adam, metrics=['accuracy'])
            else:
                model.compile(loss='categorical_crossentropy', optimizer=opt_adam, metrics=['accuracy'])
                
            print(model.summary())
        else:
            raise ValueError('You must supply a valid model file.')

    
    # We now either have a newly created and trained model or we have loaded a
    # previously trained model. We have also loaded the validation data pool so
    # we can go ahead and validate our model on this 'unseen' data...
    
    # ========================================================================
    #    Model Testing
    # ========================================================================
    fn_etc.info_msg('Testing model performance...')
    # Estimate the performance of the model, now also including unseen Testing data:
    print('Training Data:')
    loss_training, accuracy_training = model.evaluate(X_training_data, Y_training_data, verbose=1)
    print('\tAccuracy:\t%.2f%%\n\tLoss:\t\t%.2f' % (accuracy_training * 100, loss_training))

    print('Validation Data:')
    loss_validation, accuracy_validation = model.evaluate(X_validation_data, Y_validation_data, verbose=1)
    print('\tAccuracy:\t%.2f%%\n\tLoss:\t\t%.2f' % (accuracy_validation * 100, loss_validation))    

    print('Testing Data:')
    loss_testing, accuracy_testing = model.evaluate(X_testing_data, Y_testing_data, verbose=1)
    print('\tAccuracy:\t%.2f%%\n\tLoss:\t\t%.2f' % (accuracy_testing * 100, loss_testing))
    
    ### Scoring by F1, precision, and recall metrics
    X_testing_output = model.predict(X_testing_data) # spits out probabilities for classification
    # these need to be converted to something binary before we can discover true/false positives/negatives
    if len(label_names) > 2:
        X_testing_predictions = X_testing_output.argmax(axis=-1)
    else:
        X_testing_predictions = [float(np.round(x - (pred_threshold - 0.5))) for x in X_testing_output]
        # X_testing_predictions = (X_testing_output > pred_threshold).astype(int)
    
    #save the validation_output e.g. to plot a distribution of the classification probabilities
    X_truth_vs_preds = np.hstack((Y_testing_labels_raw, X_testing_output))
    np.savetxt(model_summary_fname + ' - Testing Probabilities.tsv', X_truth_vs_preds, delimiter='\t')
    
    ##
    ## histogram of the testing data scores distribution coloured by ground-truth
    ##
    
    hist_colors = ['orange', 'cyan', 'magenta', 'green'] # add more colours if you need!
    
    labels_histogram = [] # will hold the scores for each label
    labels_legend = []
    labels_colors = []
    
    for label in range(len(label_names)):
        # find the scores (for this label) for those points which should be this label
        true_label_idx = np.where(X_truth_vs_preds[:,0] == label)[0] # find the points which are truly this label
        if mdl_labels == 1:
            label_hist = X_testing_output[true_label_idx][:,0]       # find the scores for those points, binary it's all in one column
        else:
            label_hist = X_testing_output[true_label_idx][:,label]       # find the scores for those points, multilabel have scores in multicolumns
        labels_histogram.append(label_hist)
        labels_legend.append('True \'' + label_names[label] + '\'')
        labels_colors.append(hist_colors[label])

    bins = np.linspace(0, 1, 50)
    
    if len(label_names) > 2:
        bar_opacity = 0.5
    else:
        bar_opacity = 1.0
    
    fig_val_scores = plt.figure(facecolor = plotbgcolor)
    fig_val_scores.set_size_inches(10, 10)
    ax_val_scores = fig_val_scores.add_subplot(111, facecolor = plotbgcolor)
    
    plt.hist(labels_histogram, bins, histtype='bar', stacked=True, label=labels_legend, color=labels_colors)
#    
#    for label in range(len(label_names)):
#        plt.hist(labels_histogram[label], bins, histtype='bar', stacked=True, alpha = bar_opacity, label =['True \'' + label_names[label] + '\''], color = [hist_colors[label]])
    ax_val_scores.tick_params(color=plottxtcolor, labelcolor=plottxtcolor)
    for spine in ax_val_scores.spines.values():
        spine.set_edgecolor(plottxtcolor)

    plt.title('Testing Model ' + modelID + ' - Score Distribution', color=plottxtcolor)
    plt.ylabel('Frequency', color=plottxtcolor)
    plt.xlabel('Score', color=plottxtcolor)
    
    plt.legend(loc='upper center')
    fig_val_scores.tight_layout()
    plt.show()
    
    for ExportImage in ExportImageFormat:
        if ExportImage == 'png':
            transparent_bg = True
        else:
            transparent_bg = False

        fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Testing Score Distribution.' + ExportImage)
        fig_val_scores.savefig(fname_out,
                    dpi=300,
                    bbox_inches=0,
                    facecolor=plotbgcolor,
                    edgecolor='none',
                    transparent=transparent_bg)
    plt.close()
    
    if mdl_labels == 1:
        Y_testing = Y_testing_data
    else:
        # we can't use the one-hot encoded categorical Y_testing_data (used when
        # training) with sklearn for the f1, precision, and recall calcs.
        # It wants a single array of values to compare to, which we already have.
        Y_testing = Y_testing_labels_raw
        
#    fmeasure1 = f1_score(Y_testing, X_testing_predictions, average='macro') # metrics for each label and calc unweighted mean. Does not deal with label imbalances
#    fmeasure2 = f1_score(Y_testing, X_testing_predictions, average='micro') # global cals counting true pos, false neg, and false pos
    fmeasure3 = f1_score(Y_testing, X_testing_predictions, average='weighted') # calculates metric for each label and finds unweighted mean. Does not account for label imbalance
    precision = precision_score(Y_testing, X_testing_predictions, average='macro')
    recall = recall_score(Y_testing, X_testing_predictions, average='macro')

    if len(label_names) > 2:
        pred_thr_formatted = 'softmax (multiple classification)'
    else:
        pred_thr_formatted = 'threshold = ' + str(pred_threshold) + ' (binary classification)'

    print('------------------------------------------------------')
    print('Performance on testing dataset for model')
    print(modelID_full)
    print('------------------------------------------------------')
    print('Prediction method:\t' + pred_thr_formatted)
    print('F1 score:\t\t' + str(fmeasure3))
    print('Precison:\t\t' + str(precision))
    print('Recall:\t\t\t' + str(recall))
    print('------------------------------------------------------')

    training_ratio_formatted = [ '%.2f' % elem for elem in training_label_fraction ]
    validation_ratio_formatted = [ '%.2f' % elem for elem in validation_label_fraction ]
    testing_ratio_formatted = [ '%.2f' % elem for elem in testing_label_fraction ]

    with open(model_summary_fname + ' - Performance Summary.txt','a') as fstats:
        writer = csv.writer(fstats, delimiter='\t')
        writer.writerow(['------------------------------------------------------'])
        writer.writerow([modelID_full])
        writer.writerow(['------------------------------------------------------'])
        writer.writerow(['Classification Labels:', label_names])
        writer.writerow(['Prediction method: ', pred_thr_formatted])
        writer.writerow(['------------------------------------------------------'])
        writer.writerow(['Training Data:'])
        writer.writerow(['','Total Points: ', total_training_count])
        writer.writerow(['','Class Split:', training_ratio_formatted])
        writer.writerow(['','Accuracy: ', '', accuracy_training * 100])
        writer.writerow(['','Loss: ', '', '', round(loss_training, ndigits=3)])
        writer.writerow(['------------------------------------------------------'])
        writer.writerow(['Validation Data:'])
        writer.writerow(['','Total Points: ', total_validation_count])
        writer.writerow(['','Class Split:', validation_ratio_formatted])
        writer.writerow(['','Accuracy', '', accuracy_validation * 100])
        writer.writerow(['','Loss: ', '', '', round(loss_validation, ndigits=3)])
        writer.writerow(['------------------------------------------------------'])
        writer.writerow(['Testing Data:'])
        writer.writerow(['','Total Points: ', total_validation_count])
        writer.writerow(['','Class Split:', testing_ratio_formatted])
        writer.writerow(['','Accuracy', '', accuracy_validation * 100])
        writer.writerow(['','Loss: ', '', '', round(loss_validation, ndigits=3)])
        writer.writerow([''])
        writer.writerow(['','F1 Score: ', '', round(fmeasure3, ndigits=3)])
        writer.writerow(['','Precision: ', '', round(precision, ndigits=3)])
        writer.writerow(['','Recall: ', '', round(recall, ndigits=3)])
        writer.writerow(['------------------------------------------------------'])
        writer.writerow([''])

    # ========================================================================
    #    Plot Confusion Matrix
    # ========================================================================
    
    fn_etc.info_msg('Generating confusion matrices...')
    
    # Training data confusion matrix
    confmat_trn_probabilities = model.predict(X_training_data, batch_size=64)       # probability assessment of event being in cluster(~1) or not (~0)
    if len(label_names) > 2:
        confmat_trn_predictions = confmat_trn_probabilities.argmax(axis=-1)
        confusionmatrix_trn = confusion_matrix(Y_training_data.argmax(axis=-1), confmat_trn_predictions)
    else:
        confmat_trn_predictions = [float(np.round(x)) for x in confmat_trn_probabilities]   # convert probability into boolean 
        confusionmatrix_trn = confusion_matrix(Y_training_data, confmat_trn_predictions)

    # Validation data confusion matrix
    confmat_val_probabilities = model.predict(X_validation_data, batch_size=64)       # probability assessment of event being in cluster(~1) or not (~0)
    if len(label_names) > 2:
        confmat_val_predictions = confmat_val_probabilities.argmax(axis=-1)
        confusionmatrix_val = confusion_matrix(Y_validation_data.argmax(axis=-1), confmat_val_predictions)
    else:
        confmat_val_predictions = [float(np.round(x)) for x in confmat_val_probabilities]   # convert probability into boolean 
        confusionmatrix_val = confusion_matrix(Y_validation_data, confmat_val_predictions)

    # Testing data confusion matrix
    confmat_test_probabilities = model.predict(X_testing_data, batch_size=64)       # probability assessment of event being in cluster(~1) or not (~0)
    if len(label_names) > 2:
        confmat_test_predictions = confmat_test_probabilities.argmax(axis=-1)
        confusionmatrix_test = confusion_matrix(Y_testing_data.argmax(axis=-1), confmat_test_predictions)
    else:
        confmat_test_predictions = [float(np.round(x)) for x in confmat_test_probabilities]   # convert probability into boolean 
        confusionmatrix_test = confusion_matrix(Y_testing_data, confmat_test_predictions)

    # Plot absolute values confusion matrix
    fig_AbsCM_trn = fn_etc.plot_confusion_matrix(confusionmatrix_trn, classes=['NC', 'C'],
                          cmap=cmap, plottxtcolor=plottxtcolor, plotbgcolor=plotbgcolor, normalize=False,
                          title='Absolute Confusion matrix - Model ' + modelID + ' - Training')
    
    for ExportImage in ExportImageFormat:
        fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Abs Confusion Matrices (training).' + ExportImage)
        fig_AbsCM_trn.savefig(fname_out,
                    dpi=300,
                    bbox_inches=0,
                    facecolor=plotbgcolor,
                    edgecolor='none',
                    transparent=True)
    plt.close()
    
    # Plot normalized confusion matrix
    fig_NormCM_trn = fn_etc.plot_confusion_matrix(confusionmatrix_trn, classes=['NC', 'C'],
                          cmap=cmap, plottxtcolor=plottxtcolor, plotbgcolor=plotbgcolor, normalize=True,
                          title='Normalized Confusion matrix - Model ' + modelID + ' - Training')
    
    for ExportImage in ExportImageFormat:
        fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Norm Confusion Matrices (training).' + ExportImage)
        fig_NormCM_trn.savefig(fname_out,
                    dpi=300,
                    bbox_inches=0,
                    facecolor=plotbgcolor,
                    edgecolor='none',
                    transparent=True)
    plt.close()


    # Plot absolute values confusion matrix
    fig_AbsCM_val = fn_etc.plot_confusion_matrix(confusionmatrix_val, classes=['NC', 'C'],
                          cmap=cmap, plottxtcolor=plottxtcolor, plotbgcolor=plotbgcolor, normalize=False,
                          title='Absolute Confusion matrix - Model ' + modelID + ' - Validation')
    
    for ExportImage in ExportImageFormat:
        fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Abs Confusion Matrices (validation).' + ExportImage)
        fig_AbsCM_val.savefig(fname_out,
                    dpi=300,
                    bbox_inches=0,
                    facecolor=plotbgcolor,
                    edgecolor='none',
                    transparent=True)
    plt.close()
    
    # Plot normalized confusion matrix
    fig_NormCM_val = fn_etc.plot_confusion_matrix(confusionmatrix_val, classes=['NC', 'C'],
                          cmap=cmap, plottxtcolor=plottxtcolor, plotbgcolor=plotbgcolor, normalize=True,
                          title='Normalized Confusion matrix - Model ' + modelID + ' - Validation')
    
    for ExportImage in ExportImageFormat:
        fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Norm Confusion Matrices (validation).' + ExportImage)
        fig_NormCM_val.savefig(fname_out,
                dpi=300,
                bbox_inches=0,
                facecolor=plotbgcolor,
                edgecolor='none',
                transparent=True)
    plt.close()
    
    
    # Testing - Plot absolute values confusion matrix
    fig_AbsCM_test = fn_etc.plot_confusion_matrix(confusionmatrix_test, classes=['NC', 'C'],
                          cmap=cmap, plottxtcolor=plottxtcolor, plotbgcolor=plotbgcolor, normalize=False,
                          title='Absolute Confusion matrix - Model ' + modelID + ' - Testing')
    
    for ExportImage in ExportImageFormat:
        fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Abs Confusion Matrices (testing).' + ExportImage)
        fig_AbsCM_test.savefig(fname_out,
                    dpi=300,
                    bbox_inches=0,
                    facecolor=plotbgcolor,
                    edgecolor='none',
                    transparent=True)
    plt.close()
    
    # Testing - Plot normalized confusion matrix
    fig_NormCM_test = fn_etc.plot_confusion_matrix(confusionmatrix_test, classes=['NC', 'C'],
                          cmap=cmap, plottxtcolor=plottxtcolor, plotbgcolor=plotbgcolor, normalize=True,
                          title='Normalized Confusion matrix - Model ' + modelID + ' - Testing')
    
    for ExportImage in ExportImageFormat:
        fname_out = os.path.join(outputpath_model, model_summary_fname + ' - Norm Confusion Matrices (testing).' + ExportImage)
        fig_NormCM_test.savefig(fname_out,
                dpi=300,
                bbox_inches=0,
                facecolor=plotbgcolor,
                edgecolor='none',
                transparent=True)
    plt.close()
    
    fn_etc.ok_msg('Finished model training and evaluation.')
    print('The input folder was\t' + inputfolder)
    print('The output folder was\t' + outputpath_model)
    print('-------------------------------------------------\n\tCompleted!\n-------------------------------------------------')

    ### END Model Training, Validation, & Testing